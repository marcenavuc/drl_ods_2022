{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fa9052-b5f4-4f93-94cc-a414a6a4ac7a",
   "metadata": {},
   "source": [
    "# DDPG\n",
    "\n",
    "Задаем структуру аппроксимаций $\\pi^\\eta(s)$, $Q^\\theta(s,a)$ и начальные вектора параметров $\\eta$, $\\theta$.\n",
    "\n",
    "Для каждого эпизода делаем:\n",
    "\n",
    "   Пока эпизод не закончен делаем:\n",
    "\n",
    "- Находясь в состоянии $S_t$ совершаем действие\n",
    "\n",
    "    $$\n",
    "    A_t = \\pi^\\eta(S_t) + Noise,\n",
    "    $$\n",
    "\n",
    "    получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем \n",
    "    $(S_t,A_t,R_t,D_t,S_{t+1}) \\Rightarrow Memory$\n",
    "\n",
    "\n",
    "- Берем $\\{(s_i,a_i,r_i,d_i,s'_i)\\}_{i=1}^{n} \\leftarrow Memory$, определяем значения\n",
    "\n",
    "    $$\n",
    "    y_i = r_i + (1 - d_i) \\gamma Q^\\theta(s'_i,\\pi^\\eta(s'_i))\n",
    "    $$\n",
    "    функции потерь\n",
    "\n",
    "    $$\n",
    "    Loss_1(\\theta) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\big(y_i - Q^\\theta(s_i,a_i)\\big)^2,\\quad Loss_2(\\eta) = -\\frac{1}{n}\\sum\\limits_{i=1}^n Q^\\theta(s_i,\\pi^\\eta(s_i))\n",
    "    $$\n",
    "\n",
    "    и обновляем вектор параметров\n",
    "\n",
    "    $$\n",
    "    \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta Loss_1(\\theta),\\quad \\eta \\leftarrow \\eta - \\beta \\nabla_\\eta Loss_2(\\eta),\\quad \\alpha,\\beta > 0\n",
    "    $$\n",
    "\n",
    "- Уменьшаем $Noise$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb4fdb2-5023-42f3-a7ba-0def3c69aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ornstein–Uhlenbeck process (Процесс Орнштейна – Уленбека)\n",
    "\n",
    "class OUNoise:\n",
    "    def __init__(self, action_dimension, mu=0, theta=0.15, sigma=0.3):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "641708d2-a56b-4645-a5f3-b7b368a4ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class TwoLayersNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, layer1_dim, layer2_dim, output_dim, output_tanh):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, layer1_dim)\n",
    "        self.layer2 = nn.Linear(layer1_dim, layer2_dim)\n",
    "        self.layer3 = nn.Linear(layer2_dim, output_dim)\n",
    "        self.output_tanh = output_tanh\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden = self.layer1(input)\n",
    "        hidden = self.relu(hidden)\n",
    "        hidden = self.layer2(hidden)\n",
    "        hidden = self.relu(hidden)\n",
    "        output = self.layer3(hidden)\n",
    "        \n",
    "        if self.output_tanh:\n",
    "            return self.tanh(output)\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "        \n",
    "class DDPG():\n",
    "    def __init__(self, state_dim, action_dim, action_scale, noise_decrease,\n",
    "                 gamma=0.99, batch_size=64, q_lr=1e-3, pi_lr=1e-4, tau=1e-2, memory_size=100000):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_scale = action_scale\n",
    "        self.pi_model = TwoLayersNeuralNetwork(self.state_dim, 400, 300, self.action_dim, output_tanh=True)\n",
    "        self.q_model = TwoLayersNeuralNetwork(self.state_dim + self.action_dim, 400, 300, 1, output_tanh=False)\n",
    "        self.pi_target_model = deepcopy(self.pi_model)\n",
    "        self.q_target_model = deepcopy(self.q_model)\n",
    "        self.noise = OUNoise(self.action_dim)\n",
    "        self.noise_threshold = 1\n",
    "        self.noise_decrease = noise_decrease\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.q_optimazer = torch.optim.Adam(self.q_model.parameters(), lr=q_lr)\n",
    "        self.pi_optimazer = torch.optim.Adam(self.pi_model.parameters(), lr=pi_lr)\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        pred_action = self.pi_model(torch.FloatTensor(state)).detach().numpy()\n",
    "        action = self.action_scale * (pred_action + self.noise_threshold * self.noise.sample())\n",
    "        return np.clip(action, -self.action_scale, self.action_scale)\n",
    "    \n",
    "    def update_target_model(self, target_model, model, optimazer, loss):\n",
    "        optimazer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimazer.step()\n",
    "        for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "            target_param.data.copy_((1 - self.tau) * target_param.data + self.tau * param.data) \n",
    "    \n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, done, next_state])\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
    "            rewards = rewards.reshape(self.batch_size, 1)\n",
    "            dones = dones.reshape(self.batch_size, 1)\n",
    "            \n",
    "            pred_next_actions = self.action_scale * self.pi_target_model(next_states)\n",
    "            next_states_and_pred_next_actions = torch.cat((next_states, pred_next_actions), dim=1)\n",
    "            targets = rewards + self.gamma * (1 - dones) * self.q_target_model(next_states_and_pred_next_actions)\n",
    "            \n",
    "            states_and_actions = torch.cat((states, actions), dim=1)\n",
    "            temp = (self.q_model(states_and_actions) - targets.detach())\n",
    "            q_loss = torch.mean((targets.detach() - self.q_model(states_and_actions)) ** 2)\n",
    "            self.update_target_model(self.q_target_model, self.q_model, self.q_optimazer, q_loss)\n",
    "            \n",
    "            pred_actions = self.action_scale * self.pi_model(states)\n",
    "            states_and_pred_actions = torch.cat((states, pred_actions), dim=1)\n",
    "            pi_loss = - torch.mean(self.q_model(states_and_pred_actions))\n",
    "            self.update_target_model(self.pi_target_model, self.pi_model, self.pi_optimazer, pi_loss)\n",
    "            \n",
    "        if self.noise_threshold > 0:\n",
    "            self.noise_threshold = max(0, self.noise_threshold - self.noise_decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e90bfa-fa82-4c7a-9174-bcbbbead4fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marka\\anaconda3\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "C:\\Users\\marka\\anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "C:\\Users\\marka\\AppData\\Local\\Temp/ipykernel_508/192948176.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=0, total_reward=-1334.8015564398079\n",
      "episode=1, total_reward=-1525.8355889636084\n",
      "episode=2, total_reward=-1646.2047885160314\n",
      "episode=3, total_reward=-1570.9778542346894\n",
      "episode=4, total_reward=-1109.3856326426808\n",
      "episode=5, total_reward=-1307.9026756903793\n",
      "episode=6, total_reward=-1232.0521245354862\n",
      "episode=7, total_reward=-1163.0404773523933\n",
      "episode=8, total_reward=-1042.2851789260446\n",
      "episode=9, total_reward=-920.4526214979396\n",
      "episode=10, total_reward=-467.1849950729043\n",
      "episode=11, total_reward=-654.7507837418733\n",
      "episode=12, total_reward=-921.0138331281235\n",
      "episode=13, total_reward=-253.64803793336628\n",
      "episode=14, total_reward=-665.215991697096\n",
      "episode=15, total_reward=-515.7694358932431\n",
      "episode=16, total_reward=-517.7784551260695\n",
      "episode=17, total_reward=-636.1508766375658\n",
      "episode=18, total_reward=-973.3759937808969\n",
      "episode=19, total_reward=-254.72980598933873\n",
      "episode=20, total_reward=-240.9448408417896\n",
      "episode=21, total_reward=-497.75228509988625\n",
      "episode=22, total_reward=-423.4746462166968\n",
      "episode=23, total_reward=-370.6661859790418\n",
      "episode=24, total_reward=-253.54898690297176\n",
      "episode=25, total_reward=-652.2978882646207\n",
      "episode=26, total_reward=-249.93993549860187\n",
      "episode=27, total_reward=-369.96066045375625\n",
      "episode=28, total_reward=-500.5337465945716\n",
      "episode=29, total_reward=-383.1739312007369\n",
      "episode=30, total_reward=-252.85141873202033\n",
      "episode=31, total_reward=-372.75533166295037\n",
      "episode=32, total_reward=-436.768143053388\n",
      "episode=33, total_reward=-125.99603289762011\n",
      "episode=34, total_reward=-365.3906055340009\n",
      "episode=35, total_reward=-279.91128609867445\n",
      "episode=36, total_reward=-383.0143582515532\n",
      "episode=37, total_reward=-334.8088567291883\n",
      "episode=38, total_reward=-120.25158074703363\n",
      "episode=39, total_reward=-382.18058789041856\n",
      "episode=40, total_reward=-250.5397282142206\n",
      "episode=41, total_reward=-432.9584970670813\n",
      "episode=42, total_reward=-250.64640029356002\n",
      "episode=43, total_reward=-256.09617054913156\n",
      "episode=44, total_reward=-496.36651380601796\n",
      "episode=45, total_reward=-518.6901200895971\n",
      "episode=46, total_reward=-373.5148570083217\n",
      "episode=47, total_reward=-602.4582444145913\n",
      "episode=48, total_reward=-250.69988270831428\n",
      "episode=49, total_reward=-133.25739553547024\n",
      "episode=50, total_reward=-371.781552821428\n",
      "episode=51, total_reward=-385.25392785050616\n",
      "episode=52, total_reward=-259.1970851251938\n",
      "episode=53, total_reward=-510.14769005843056\n",
      "episode=54, total_reward=-383.2229533653449\n",
      "episode=55, total_reward=-255.24715342361006\n",
      "episode=56, total_reward=-366.0078730933869\n",
      "episode=57, total_reward=-250.8010722180133\n",
      "episode=58, total_reward=-542.9245246962777\n",
      "episode=59, total_reward=-125.8832016854668\n",
      "episode=60, total_reward=-251.30568261180292\n",
      "episode=61, total_reward=-128.50192208700824\n",
      "episode=62, total_reward=-122.37498855506708\n",
      "episode=63, total_reward=-478.5954487762839\n",
      "episode=64, total_reward=-3.5024786056869557\n",
      "episode=65, total_reward=-124.09493319493502\n",
      "episode=66, total_reward=-236.0896764036418\n",
      "episode=67, total_reward=-128.4365902055771\n",
      "episode=68, total_reward=-393.9906510947274\n",
      "episode=69, total_reward=-660.5951870005797\n",
      "episode=70, total_reward=-498.99864561761524\n",
      "episode=71, total_reward=-254.34265819208565\n",
      "episode=72, total_reward=-255.23531501013028\n",
      "episode=73, total_reward=-253.87973112035058\n",
      "episode=74, total_reward=-746.3884621614903\n",
      "episode=75, total_reward=-253.9532859181776\n",
      "episode=76, total_reward=-495.3906540646973\n",
      "episode=77, total_reward=-379.0836257275963\n",
      "episode=78, total_reward=-250.27421770918804\n",
      "episode=79, total_reward=-135.69847467965616\n",
      "episode=80, total_reward=-130.98097437131642\n",
      "episode=81, total_reward=-250.37266313856014\n",
      "episode=82, total_reward=-132.5597710140781\n",
      "episode=83, total_reward=-365.758804527896\n",
      "episode=84, total_reward=-137.25718942822365\n",
      "episode=85, total_reward=-638.3375149137798\n",
      "episode=86, total_reward=-511.22126119087625\n",
      "episode=87, total_reward=-142.73071863375577\n",
      "episode=88, total_reward=-258.19591172430614\n",
      "episode=89, total_reward=-382.526350409261\n",
      "episode=90, total_reward=-248.63732801293057\n",
      "episode=91, total_reward=-369.3986570209055\n",
      "episode=92, total_reward=-505.08604316400607\n",
      "episode=93, total_reward=-256.97922433289244\n",
      "episode=94, total_reward=-515.8212495609522\n",
      "episode=95, total_reward=-248.2235477803316\n",
      "episode=96, total_reward=-379.0378818998466\n",
      "episode=97, total_reward=-131.8111621970661\n",
      "episode=98, total_reward=-257.0514430112558\n",
      "episode=99, total_reward=-487.71403438539716\n",
      "episode=100, total_reward=-407.2295792858166\n",
      "episode=101, total_reward=-260.99043932753784\n",
      "episode=102, total_reward=-308.1229860757925\n",
      "episode=103, total_reward=-367.9875437479698\n",
      "episode=104, total_reward=-127.59497264975865\n",
      "episode=105, total_reward=-131.98778755525032\n",
      "episode=106, total_reward=-258.04315174099105\n",
      "episode=107, total_reward=-134.227857060624\n",
      "episode=108, total_reward=-258.5330213843494\n",
      "episode=109, total_reward=-584.3139966056485\n",
      "episode=110, total_reward=-370.7226758097223\n",
      "episode=111, total_reward=-527.7073038329179\n",
      "episode=112, total_reward=-377.01905581600147\n",
      "episode=113, total_reward=-377.98900768122775\n",
      "episode=114, total_reward=-366.9144599633196\n",
      "episode=115, total_reward=-251.29368017968977\n",
      "episode=116, total_reward=-126.08412342601993\n",
      "episode=117, total_reward=-514.3731249247296\n",
      "episode=118, total_reward=-252.8017842911659\n",
      "episode=119, total_reward=-133.8680913921473\n",
      "episode=120, total_reward=-424.79048305668647\n",
      "episode=121, total_reward=-384.5113493119402\n",
      "episode=122, total_reward=-401.9304446892351\n",
      "episode=123, total_reward=-254.68846978304745\n",
      "episode=124, total_reward=-542.3128141051885\n",
      "episode=125, total_reward=-139.44240638865483\n",
      "episode=126, total_reward=-301.5179121046963\n",
      "episode=127, total_reward=-312.7666665135378\n",
      "episode=128, total_reward=-138.99241329677918\n",
      "episode=129, total_reward=-257.61742037081183\n",
      "episode=130, total_reward=-257.05611843830945\n",
      "episode=131, total_reward=-12.792414108632729\n",
      "episode=132, total_reward=-133.98656261092188\n",
      "episode=133, total_reward=-370.3499704664246\n",
      "episode=134, total_reward=-244.72969993394184\n",
      "episode=135, total_reward=-135.76478051091172\n",
      "episode=136, total_reward=-246.80246500611534\n",
      "episode=137, total_reward=-253.26361562153662\n",
      "episode=138, total_reward=-256.0066274105282\n",
      "episode=139, total_reward=-128.66087991914256\n",
      "episode=140, total_reward=-11.38982330542417\n",
      "episode=141, total_reward=-12.68130341694145\n",
      "episode=142, total_reward=-14.756426971042304\n",
      "episode=143, total_reward=-126.75689279451113\n",
      "episode=144, total_reward=-254.3035742233642\n",
      "episode=145, total_reward=-261.97229046001274\n",
      "episode=146, total_reward=-361.37905001564263\n",
      "episode=147, total_reward=-301.81489098108347\n",
      "episode=148, total_reward=-134.06127954410977\n",
      "episode=149, total_reward=-13.697490108457286\n",
      "episode=150, total_reward=-248.61927432881996\n",
      "episode=151, total_reward=-246.94545312799806\n",
      "episode=152, total_reward=-252.79998337871496\n",
      "episode=153, total_reward=-249.01988436838218\n",
      "episode=154, total_reward=-134.58903055242808\n",
      "episode=155, total_reward=-128.7574243436616\n",
      "episode=156, total_reward=-12.751964994975895\n",
      "episode=157, total_reward=-387.56622276389663\n",
      "episode=158, total_reward=-129.4691898612371\n",
      "episode=159, total_reward=-359.92013029102924\n",
      "episode=160, total_reward=-274.38252442234483\n",
      "episode=161, total_reward=-250.34923406234867\n",
      "episode=162, total_reward=-247.53736276047928\n",
      "episode=163, total_reward=-134.73145398788648\n",
      "episode=164, total_reward=-131.98246019358876\n",
      "episode=165, total_reward=-131.11180137183572\n",
      "episode=166, total_reward=-274.7872964533091\n",
      "episode=167, total_reward=-127.4155829259839\n",
      "episode=168, total_reward=-260.50538934254325\n",
      "episode=169, total_reward=-125.07394653216666\n",
      "episode=170, total_reward=-137.15058896370113\n",
      "episode=171, total_reward=-10.014331666692852\n",
      "episode=172, total_reward=-139.93855537374637\n",
      "episode=173, total_reward=-123.50188295286677\n",
      "episode=174, total_reward=-127.3714953406534\n",
      "episode=175, total_reward=-277.2522678830539\n",
      "episode=176, total_reward=-253.64562080382683\n",
      "episode=177, total_reward=-245.37661880260183\n",
      "episode=178, total_reward=-131.28006052659424\n",
      "episode=179, total_reward=-243.78515668835306\n",
      "episode=180, total_reward=-122.0154257243144\n",
      "episode=181, total_reward=-7.285207498905535\n",
      "episode=182, total_reward=-129.58126856567452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=183, total_reward=-127.69234814060512\n",
      "episode=184, total_reward=-132.2390588999736\n",
      "episode=185, total_reward=-250.32010098059897\n",
      "episode=186, total_reward=-331.081512141525\n",
      "episode=187, total_reward=-294.3771293976741\n",
      "episode=188, total_reward=-7.186292460677919\n",
      "episode=189, total_reward=-6.910015969813687\n",
      "episode=190, total_reward=-127.77939049614344\n",
      "episode=191, total_reward=-131.00873419896453\n",
      "episode=192, total_reward=-6.887756208577514\n",
      "episode=193, total_reward=-242.7954928830705\n",
      "episode=194, total_reward=-125.16738509460522\n",
      "episode=195, total_reward=-247.03015600011346\n",
      "episode=196, total_reward=-133.06972137111734\n",
      "episode=197, total_reward=-7.525640842115034\n",
      "episode=198, total_reward=-309.01432746184423\n",
      "episode=199, total_reward=-124.90500236960108\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "episode_n = 200\n",
    "trajectory_len = 200\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "agent = DDPG(state_dim=3, action_dim=1, action_scale=2, noise_decrease = 1 / (episode_n * trajectory_len))\n",
    "\n",
    "for episode in range(episode_n):\n",
    "    \n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for _ in range(trajectory_len):\n",
    "        action = agent.get_action(state)\n",
    "        next_action, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        agent.fit(state, action, reward, done, next_action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_action\n",
    "    \n",
    "    print(f'episode={episode}, total_reward={total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8d8d1-f028-4df9-ae2f-28825827fcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}